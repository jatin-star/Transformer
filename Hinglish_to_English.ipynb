{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "UiuQHYM2r3DA"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Specify the path to your JSON file\n",
    "# json_file_path = '/content/drive/MyDrive/Datasets/hinglish_upload_v1.json'\n",
    "# csv_file_path = '/content/drive/MyDrive/Datasets/hinglish.csv'\n",
    "# # Read the JSON file into a DataFrame\n",
    "# df = pd.read_json(json_file_path, lines=True)\n",
    "# new_df = df['translation'].apply(pd.Series)\n",
    "\n",
    "# new_df[['hi_ng', 'en']].to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "5xBbzG2EjOJG"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "sequence_length = 30\n",
    "batch_size = 128\n",
    "validation_split = 0.15\n",
    "embed_dim = 256\n",
    "latent_dim = 256\n",
    "num_heads = 8\n",
    "epochs = 30 # Number of Epochs to train\n",
    "is_training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "uWwUQgtsjR77"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data = pd.read_csv('hinglish.csv')[:5000]\n",
    "data = data.rename(columns={'hi_ng': 'english', 'en': 'spanish'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5NzHDp2_jXyp",
    "outputId": "4ab23544-0748-4fda-8284-dbae34079222"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film ka kya naam hai</td>\n",
       "      <td>What's the name of the movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>namaste, sada hua tomatoes score mahaan hai, l...</td>\n",
       "      <td>Hi, the rotten tomatoes score is great but the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kya aapako lagata hai ki aapako film pasand aa...</td>\n",
       "      <td>Do you think you will like the movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yah kis tarah kee philm hai</td>\n",
       "      <td>What kind of movie is it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>film  kab banee thee?</td>\n",
       "      <td>when was the movie made?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                               film ka kya naam hai   \n",
       "1  namaste, sada hua tomatoes score mahaan hai, l...   \n",
       "2  kya aapako lagata hai ki aapako film pasand aa...   \n",
       "3                        yah kis tarah kee philm hai   \n",
       "4                              film  kab banee thee?   \n",
       "\n",
       "                                             spanish  \n",
       "0                       What's the name of the movie  \n",
       "1  Hi, the rotten tomatoes score is great but the...  \n",
       "2               Do you think you will like the movie  \n",
       "3                           What kind of movie is it  \n",
       "4                           when was the movie made?  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLZ6Im37sUBb",
    "outputId": "b160a9d5-5c9e-4501-f999-aed7c589ee5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "-FCXH4_QuEt4"
   },
   "outputs": [],
   "source": [
    "# combined_text = ' '.join(col for col in data['spanish'])\n",
    "# len(set(combined_text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "4MFJo_hCWiYB"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAoBx0wdjYVQ",
    "outputId": "8ad9fca5-cd78-41a4-e9f3-0f7ae50b904d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:2.10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(f\"Tensorflow Version:{tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "ZEmsPTBrjZHK"
   },
   "outputs": [],
   "source": [
    "data[\"spanish\"] = data[\"spanish\"].apply(lambda item: \"[start] \" + item + \" [end]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ly-f48FKjY8w",
    "outputId": "cbd73c06-f07c-4abe-db19-e93b8050f0f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film ka kya naam hai</td>\n",
       "      <td>[start] What's the name of the movie [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>namaste, sada hua tomatoes score mahaan hai, l...</td>\n",
       "      <td>[start] Hi, the rotten tomatoes score is great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kya aapako lagata hai ki aapako film pasand aa...</td>\n",
       "      <td>[start] Do you think you will like the movie [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yah kis tarah kee philm hai</td>\n",
       "      <td>[start] What kind of movie is it [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>film  kab banee thee?</td>\n",
       "      <td>[start] when was the movie made? [end]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                               film ka kya naam hai   \n",
       "1  namaste, sada hua tomatoes score mahaan hai, l...   \n",
       "2  kya aapako lagata hai ki aapako film pasand aa...   \n",
       "3                        yah kis tarah kee philm hai   \n",
       "4                              film  kab banee thee?   \n",
       "\n",
       "                                             spanish  \n",
       "0         [start] What's the name of the movie [end]  \n",
       "1  [start] Hi, the rotten tomatoes score is great...  \n",
       "2  [start] Do you think you will like the movie [...  \n",
       "3             [start] What kind of movie is it [end]  \n",
       "4             [start] when was the movie made? [end]  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfMurHUajY5P",
    "outputId": "299dfe3b-8209-43a5-93ad-c6cd2b013139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@\\^_`{|}~¿\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "print(strip_chars)\n",
    "def spanish_standardize(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\n",
    "english_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "spanish_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=spanish_standardize,\n",
    ")\n",
    "english_vectorization.adapt(list(data[\"english\"]))\n",
    "spanish_vectorization.adapt(list(data[\"spanish\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "rFxmP67ab9Vo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_vectorization_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_vectorization_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: spanish_vectorization_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: spanish_vectorization_model\\assets\n"
     ]
    }
   ],
   "source": [
    "english_vectorization_model = tf.keras.Sequential([english_vectorization])\n",
    "spanish_vectorization_model = tf.keras.Sequential([spanish_vectorization])\n",
    "\n",
    "# Adapt the vectorization layers\n",
    "english_vectorization_model.layers[0].adapt(list(data[\"english\"]))\n",
    "spanish_vectorization_model.layers[0].adapt(list(data[\"spanish\"]))\n",
    "\n",
    "# Save the models\n",
    "english_vectorization_model.save(\"english_vectorization_model\")\n",
    "spanish_vectorization_model.save(\"spanish_vectorization_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "english_vectorization_model = load_model(\"english_vectorization_model\")\n",
    "spanish_vectorization_model = load_model(\n",
    "    \"spanish_vectorization_model\",\n",
    "    custom_objects={'spanish_standardize': spanish_standardize}\n",
    ")\n",
    "\n",
    "# Extract the TextVectorization layers\n",
    "english_vectorization = english_vectorization_model.layers[0]\n",
    "spanish_vectorization = spanish_vectorization_model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       "array([ 52,  13,   9, 149,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vectorization(\"film ka kya naam hai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "xhTNnBPRjY3M"
   },
   "outputs": [],
   "source": [
    "def preprocess(english, spanish):\n",
    "    english = english_vectorization(english)\n",
    "    spanish = spanish_vectorization(spanish)\n",
    "    return ({\"encoder_inputs\": english, \"decoder_inputs\": spanish[:, :-1]}, spanish[:, 1:])\n",
    "def make_dataset(df, batch_size, mode):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(df[\"english\"]), list(df[\"spanish\"])))\n",
    "    if mode == \"train\":\n",
    "       dataset = dataset.shuffle(batch_size * 4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5hUAd9AjY12",
    "outputId": "f770304c-ddd9-4fd9-ee6b-c16c1cf301d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4250, 2), (750, 2))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, valid = train_test_split(data, test_size=validation_split, random_state=42)\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "gCcP1ft6jYz7"
   },
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train, batch_size=batch_size, mode=\"train\")\n",
    "valid_ds = make_dataset(valid, batch_size=batch_size, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btkqzGOTjYyK",
    "outputId": "ff4a999c-d612-4d14-fa89-0dc3789abe5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'encoder_inputs': <tf.Tensor: shape=(128, 30), dtype=int64, numpy=\n",
      "array([[ 643,  200,   17, ...,    0,    0,    0],\n",
      "       [6725,    0,    0, ...,    0,    0,    0],\n",
      "       [ 536, 5881,   65, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 103,  155,   21, ...,    0,    0,    0],\n",
      "       [ 141,  288,  308, ...,    0,    0,    0],\n",
      "       [  25,   16,   48, ...,    0,    0,    0]], dtype=int64)>, 'decoder_inputs': <tf.Tensor: shape=(128, 30), dtype=int64, numpy=\n",
      "array([[   2, 2109,    6, ...,    0,    0,    0],\n",
      "       [   2, 4823,    3, ...,    0,    0,    0],\n",
      "       [   2,   75,  208, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   2,   75,   37, ...,    0,    0,    0],\n",
      "       [   2,  131,    5, ...,  958,    5,   23],\n",
      "       [   2,    5,   23, ...,    0,    0,    0]], dtype=int64)>}, <tf.Tensor: shape=(128, 30), dtype=int64, numpy=\n",
      "array([[2109,    6,    9, ...,    0,    0,    0],\n",
      "       [4823,    3,    0, ...,    0,    0,    0],\n",
      "       [  75,  208,   28, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  75,   37,    5, ...,    0,    0,    0],\n",
      "       [ 131,    5,  565, ...,    5,   23,    3],\n",
      "       [   5,   23,   27, ...,    0,    0,    0]], dtype=int64)>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "-rakX-iIj55F"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "9Lcs_uegj5yk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embed_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length,\n",
    "            output_dim=embed_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = self.token_embeddings.compute_mask(inputs)\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "oAR-Lp80j5wt"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(latent_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask = None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        padding_mask = None\n",
    "        if mask != None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask\n",
    "        )\n",
    "\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask\n",
    "        )\n",
    "\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        out = self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aWVyjUiqj5vB",
    "outputId": "e196e6f9-9722-4dce-fc8e-d4eddb2ea1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder_inputs: (None, 30)\n",
      "Shape of positional_encoder_inputs: (None, 30, 256)\n",
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_18 (Posit  (None, 30, 256)     2567680     ['encoder_inputs[0][0]']         \n",
      " ionalEmbedding)                                                                                  \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_9 (Transfo  (None, 30, 256)     2236160     ['positional_embedding_18[0][0]']\n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " model_19 (Functional)          (None, 30, 10000)    9477904     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_9[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,281,744\n",
      "Trainable params: 14,281,744\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def edit_distance(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1, output_type=y_true.dtype)\n",
    "    y_true_tensor =  tf.sparse.from_dense(\n",
    "        y_true\n",
    "    )\n",
    "    y_pred_tensor = tf.sparse.from_dense(\n",
    "        y_pred\n",
    "    )\n",
    "    metric = 1 - tf.edit_distance(y_true_tensor, y_pred_tensor, normalize=True)\n",
    "    return metric\n",
    "\n",
    "def get_transformer():\n",
    "    encoder_inputs = tf.keras.Input(shape=(sequence_length, ), name=\"encoder_inputs\")\n",
    "    print(\"Shape of encoder_inputs:\", encoder_inputs.shape)\n",
    "\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "    print(\"Shape of positional_encoder_inputs:\", x.shape)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, num_heads, latent_dim)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    transformer = keras.Model(\n",
    "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    "    )\n",
    "    '''transformer.compile(\n",
    "        \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\", edit_distance]\n",
    "    )'''\n",
    "    transformer.compile(\n",
    "        \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return transformer\n",
    "transformer = get_transformer()\n",
    "transformer.summary()\n",
    "keras.utils.plot_model(transformer, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNUSf7iJj5tk",
    "outputId": "6dd24130-9c71-4c70-9bc9-43690f68216b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/34 [==========>...................] - ETA: 4s - loss: 0.0316 - accuracy: 0.9785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.fit(train_ds, epochs=100, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "transformer.save_weights(\"transformer_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recreate the model architecture\n",
    "# transformer = get_transformer()  # Make sure to redefine the get_transformer function as needed\n",
    "\n",
    "# # Load the weights\n",
    "# transformer.load_weights(\"transformer_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "VH6KWhWYj5rw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 91ms/step - loss: 3.2109 - accuracy: 0.3274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.2109458446502686, 0.32739168405532837]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.evaluate(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "rrLscbhJj5pv"
   },
   "outputs": [],
   "source": [
    "spanish_vocab = spanish_vectorization.get_vocabulary()\n",
    "spanish_index_lookup = dict(zip(range(len(spanish_vocab)), spanish_vocab))\n",
    "def remove_start_and_end_token(sentence):\n",
    "    return sentence.replace(\"[start] \", \"\").replace(\" [end]\", \"\")\n",
    "def decode_sequence(transformer, input_sentence):\n",
    "    tokenized_input_sentence = english_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(sequence_length):\n",
    "        tokenized_target_sentence = spanish_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spanish_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return remove_start_and_end_token(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "MQvldmJgkIvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: are tuje pasand nahi tho kya tu koi recommend nahi kartha war movies ko.. ya dusra kya reason hey yar\n",
      "Spanish: are you  not recommending it just because you personally don't like war movies or for another reason?\n",
      "Translated: are you not recommending it just because you personally dont like war movies or for another reason\n",
      "English: muje patha nahi hein. lekin voh comedian type ka actor hein\n",
      "Spanish: I am not sure about that. He seems to be more of a comedian type actor. \n",
      "Translated: i am not sure about that he seems to be more of a comedian type actor\n",
      "English: hello, kaise ho aap? kya aapko Iron man movie pasand hein?\n",
      "Spanish: Hello how are you? How did you like the movie Iron Man? \n",
      "Translated: hello how are you how did you like the movie iron man\n",
      "English: sad hein yar\n",
      "Spanish: That is sad.\n",
      "Translated: that is sad\n",
      "English: ye wali kafi achi hai but kafi visual hai\n",
      "Spanish: That one is really good but its really visual \n",
      "Translated: that one is really good but its really visual\n",
      "English: tumhara kya khayal hai?\n",
      "Spanish: What did you thinK?\n",
      "Translated: what did you think\n",
      "English: Inception ka bhi rotten tomatoes par score 86% hai, jo ki kafi high hai.\n",
      "Spanish: Inception also has a rotten tomatoes score of 86% which is really high.\n",
      "Translated: inception also has a rotten tomatoes score of 86 which is really high\n",
      "English: aww, acha, maybe dusre time meine flix mein dekh lunga, jab mein bored hota lol\n",
      "Spanish: Aww, I see, ok. maybe ill catch it sometime on the flix when im bored af lol\n",
      "Translated: aww i see ok maybe ill catch it sometime on the flix when im bored af lol\n",
      "English: mujhe hailoveen ke aasapaas is tarah kee philm pasand hai; anyatha, mujhe ekshan philmon ka aanand milata hai\n",
      "Spanish: I like this type of movie around Halloween; otherwise, I enjoy action films.\n",
      "Translated: i like this type of movie around halloween otherwise i enjoy action films\n",
      "English: Mera matlab hai ki main use pure Philip ki live-action ke liye credit deta huin, jo ki sirf Maleficient ki cheezon se Aurora ko bachate hain, but yahan tak ki computers aur live-action ke saath, in mein se kisi ne bhi animated version ke saath justice nhi ki, jahan wo dragon mein badal jati hai\n",
      "Spanish: I mean I do give them credit for the live-action of the whole Phillip rescuing Aurora from Maleficent thing, but even with computers and the live-action, none of it did justice to the animated version where she turns into the dragon\n",
      "Translated: i mean i do give them credit for the liveaction of the whole phillip rescuing aurora from maleficent thing but even with computers and the liveaction none of it did\n"
     ]
    }
   ],
   "source": [
    "reference_sentences =[]\n",
    "decoded_sentences = []  # list of decoded Spanish sentences\n",
    "for i in np.random.choice(len(data), 10):\n",
    "    item = data.iloc[i]\n",
    "    translated = decode_sequence(transformer, item[\"english\"])\n",
    "    print(\"English:\", remove_start_and_end_token(item[\"english\"]))\n",
    "    print(\"Spanish:\", remove_start_and_end_token(item[\"spanish\"]))\n",
    "    print(\"Translated:\", translated)\n",
    "    reference_sentences.append(remove_start_and_end_token(item[\"spanish\"]))\n",
    "    decoded_sentences.append(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im doing well\n"
     ]
    }
   ],
   "source": [
    "translated = decode_sequence(transformer, \"I am late today\")\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "6fLBoM59WJU0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU score: 77.42\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n",
    "corpus_score = corpus_bleu([[ref] for ref in reference_sentences], decoded_sentences)\n",
    "\n",
    "print(f\"Corpus BLEU score: {corpus_score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
